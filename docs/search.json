[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Minimal Quarto Book",
    "section": "",
    "text": "Acknowledgments\ndef¬†7.19\ndef¬†7.3\nFrom Prophecy to Prediction: The Myth Inspiring Pythia‚Äôs Advice\nThe Pythia‚Äôs Advice project is dedicated to the art of sales forecasting. Drawing inspiration from the ancient myth of the Oracle of Delphi, where the Pythia‚Äîthe high priestess‚Äîdelivered cryptic prophecies, we harness the power of Data Science‚Äîour modern-day Pythia‚Äîto foresee future trends. Just as the Pythia‚Äôs prophecies required careful interpretation, our statistical and machine learning techniques produce Advice that requires human expertise for effective application. By embodying this union of technology and human insight, we aptly named our project Pythia‚Äôs Advice. This synergy ensures well-informed, data-driven decision-making and enhances the operational efficiency of the supply chain.\nAccording to ancient myth, the Pythia inhaled vapors emanating from the remnants of the Python slain by Apollo‚Äîthe Olympian god of the sun, music, and prophecy. These vapors induced a trance-like state that allowed her to channel his prophetic insights, revealing future events. Similarly, our project ‚Äòinhales‚Äô vast amounts of data‚Äîour ‚ÄòPython‚Äôs Vapors‚Äô‚Äîwhich, when processed with complex algorithms, enable us to decipher hidden patterns and unveil valuable foresight. However, these predictive insights must be carefully interpreted to avoid missteps‚Äîmuch like when King Croesus misread the Pythia‚Äôs prophecy that ‚Äòa great empire will fall.‚Äô He waged war and, in doing so, opened Pandora‚Äôs Box‚Äîleading to the downfall of his own great empire‚Äîthe Kingdom of Lydia.\nFurthermore, Pythia‚Äôs Advice emphasizes the crucial bond between humans and nature. By focusing on the sales forecast of food for biodiversity‚Äîproducts that support ecological diversity‚Äîwe aim to benefit human health and promote planetary well-being. This harmonious relationship mirrors the Pythia‚Äôs sacred connection with the divine, symbolizing a balance between technological capabilities and the natural world.\nOur logo, featuring a tree integrated with the Pythia through conductive traces against an enlightening sunset that symbolizes guidance and inspiration, beautifully encapsulates this synergy. This imagery symbolizes the fusion of natural wisdom and technological innovation. It reminds us that while we rely on cutting-edge technology to predict the future, our roots remain deeply intertwined with nature.\nThrough Pythia‚Äôs Advice, we honor the legacy of the Oracle of Delphi, blending ancient wisdom with modern technology to illuminate the path forward.\nI am grateful for the insightful comments offered by Ieke le Blanc. These have improved this study in innumerable ways and saved me from many errors; those that inevitably remain are entirely my own responsibility.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Pythia's Advice</span>"
    ]
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "My Minimal Quarto Book",
    "section": "1.1 Methodology",
    "text": "1.1 Methodology\nThis research follows the CRISP-ML methodology, (Costa, 2022) as the guiding framework.\nCRISP-ML is an acronym for ‚ÄúCross-Industry Standard Process for Machine Learning.‚Äù It is a systematic framework for organizing and executing machine learning projects. The methodology includes six key steps:\n\nunderstanding the problem\npreparing the data\nselecting and tuning models\nevaluating performance\ndeploying the solution\nmonitoring and maintaining the model.\n\nThis thesis focuses on the first 4 steps, the results will be taken as advice to be implemented in the current way of working.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Pythia's Advice</span>"
    ]
  },
  {
    "objectID": "index.html#terminology",
    "href": "index.html#terminology",
    "title": "My Minimal Quarto Book",
    "section": "Terminology",
    "text": "Terminology\n\ndef 7.1 BBD:\nBest Before Date\n\n\ndef 7.2 GSL:\nGuaranteed Shelf Life\n\n\ndef 7.3 OOD:\nOut of Date\n\n\ndef 7.4 OOS:\nOut of Stock\n\n\ndef 7.5 SKU:\nStock Keeping Unit",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Pythia's Advice</span>"
    ]
  },
  {
    "objectID": "index.html#entities",
    "href": "index.html#entities",
    "title": "My Minimal Quarto Book",
    "section": "Entities",
    "text": "Entities\n\ndef 7.6 MSO:\nMarketing & Sales Organization\n\n\ndef 7.7 HFS:\nHealth Food Store\n\n\ndef 7.8 OOH:\nOut of Home",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Pythia's Advice</span>"
    ]
  },
  {
    "objectID": "nb/02_data_understanding_rev.html",
    "href": "nb/02_data_understanding_rev.html",
    "title": "2¬† Data Understanding",
    "section": "",
    "text": "2.1 Data Collection Report",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Understanding</span>"
    ]
  },
  {
    "objectID": "nb/02_data_understanding_rev.html#data-description-report",
    "href": "nb/02_data_understanding_rev.html#data-description-report",
    "title": "2¬† Data Understanding",
    "section": "2.2 Data Description Report",
    "text": "2.2 Data Description Report",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Understanding</span>"
    ]
  },
  {
    "objectID": "nb/02_data_understanding_rev.html#data-exploration-report",
    "href": "nb/02_data_understanding_rev.html#data-exploration-report",
    "title": "2¬† Data Understanding",
    "section": "2.3 Data Exploration Report",
    "text": "2.3 Data Exploration Report",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Understanding</span>"
    ]
  },
  {
    "objectID": "nb/02_data_understanding_rev.html#data-quality-report",
    "href": "nb/02_data_understanding_rev.html#data-quality-report",
    "title": "2¬† Data Understanding",
    "section": "2.4 Data Quality Report",
    "text": "2.4 Data Quality Report",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Understanding</span>"
    ]
  },
  {
    "objectID": "nb/02_data_understanding_rev.html#identification-of-data-sources",
    "href": "nb/02_data_understanding_rev.html#identification-of-data-sources",
    "title": "2¬† Data Understanding",
    "section": "2.5 Identification of Data Sources",
    "text": "2.5 Identification of Data Sources",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Understanding</span>"
    ]
  },
  {
    "objectID": "nb/02_data_understanding_rev.html#data-quality-assessment",
    "href": "nb/02_data_understanding_rev.html#data-quality-assessment",
    "title": "2¬† Data Understanding",
    "section": "2.6 Data Quality Assessment",
    "text": "2.6 Data Quality Assessment\nassessing the - completeness: impute missing data - accuracy: = consistency:\n\n2.6.1 Data Description\nA descriptive exploratory analysis describes the data by its statistical properties and metadata.\noutliers?\n\n\n2.6.2 Data Verification",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Understanding</span>"
    ]
  },
  {
    "objectID": "nb/02_data_understanding_rev.html#sdmfrcac-fa",
    "href": "nb/02_data_understanding_rev.html#sdmfrcac-fa",
    "title": "2¬† Data Understanding",
    "section": "3.1 SDMFRCAC FA",
    "text": "3.1 SDMFRCAC FA",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Understanding</span>"
    ]
  },
  {
    "objectID": "nb/03_data_preparation_rev.html",
    "href": "nb/03_data_preparation_rev.html",
    "title": "3¬† Data Preparation",
    "section": "",
    "text": "4 Data Pipelines\nSee Figure Figure¬†4.1 for the thumbnail overview.",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "nb/03_data_preparation_rev.html#work-flow-architecture",
    "href": "nb/03_data_preparation_rev.html#work-flow-architecture",
    "title": "3¬† Data Preparation",
    "section": "5.1 Work Flow & Architecture :",
    "text": "5.1 Work Flow & Architecture :\nNeed for a a robust, scalable and future-proof architecture with the flexibility to adapt as the data sets evolves, all while maintaining excellent performance and usability.\n\nNeed for fast startup and in-memory processing.\nScalable for future growth\nMinimal in complexity, leveraging familiar tools and lightweight components.\n\nVertically Scalable: The architecture leverages your node‚Äôs 24 GB of memory effectively, with room for growth by upgrading hardware (e.g., adding more RAM or CPU cores).\nLow Complexity Overhead:\nDuckDB introduces minimal complexity since it integrates seamlessly with both R and Python. SQL provides a familiar and powerful tool for managing data subsets, making the workflow easy to maintain. Prepared for Growth:\nThe combination of Feather and DuckDB ensures you can handle current and moderate future data sizes efficiently. For substantial growth, DuckDB can work with Parquet files or other scalable formats with minimal changes. Optimized for Performance:\nFeather ensures fast data access at startup. DuckDB allows efficient on-disk querying to reduce memory overhead, ensuring smooth performance even as datasets grow.\n\n5.1.1 \nThis workflow is ideal when working with datasets that are manageable in size and can be fully loaded into memory for analysis. Incorporating DuckDB adds the flexibility of SQL querying, which can be advantageous for preprocessing data before intensive computations. The use of Feather format ensures fast data loading, which is important for applications where user wait time during startup should be minimized.\nIf your dataset grows or your scalability needs change, you might need to adjust the workflow accordingly, possibly by switching to Parquet files and leveraging DuckDB‚Äôs capabilities to handle larger datasets more efficiently.\nArchitecture\nScalability:\n\nadjust your SQL queries to limit the data loaded into memory, ensuring your application remains performant.\nscale vertically with increased RAM and CPU cores to manage larger Feather files and more complex algorithms without architectural changes.\n\nPerformance:\n\nFeather enables fast reads for smaller datasets during startup.\nDuckDB efficiently handles larger datasets by querying on disk and loading only the necessary data into memory.\n\n\n\n5.1.2 Data Stageing\n\nD00: contains the BW OpenHub Export, CSV - same as AL11\n\n\n\n5.1.3 Data Ingestion:\nOccurs once a month, converting CSV to Parquet (or another format). header is added and stored in D01\n\n\n5.1.4 Data Access and Querying:\n\nData is imported into R data.table objects via DuckDB, which reads the stored files.\nQueries are often simple SQL SELECT statements on one table or basic INNER JOINs.\nThe same SQL queries are used to import data into Python using Polars via DuckDB.\n\n\n\n5.1.5 Data Manipulation:\n\nFurther data manipulation in R is done using data.table.\nIn Python, data manipulation is performed using Polars.\n\nEvaluation of Your Proposed Workflow\nGiven your familiarity with SQL and DuckDB, and your need for scalability, your proposed workflow is sound and aligns well with your goals.\n\n\n5.1.6 Advantages of Your Workflow\n\nScalability\n\nEfficient Data Storage: Parquet files are columnar, compressed, and optimized for performance, which is beneficial for large datasets.\nQuery Optimization: DuckDB is designed for efficient analytical queries, even on large datasets.\n\nFlexibility\n\nLanguage Agnostic: DuckDB can be used within R and Python, allowing seamless transition between languages.\nSQL Familiarity: Your proficiency in SQL means you can leverage DuckDB‚Äôs SQL interface effectively.\n\nPerformance\n\n\nFast Query Execution: DuckDB is optimized for OLAP workloads and can execute complex queries quickly.\nDirect Parquet Support: DuckDB can read Parquet files directly without the need to load the entire dataset into memory.\n\n\nIntegration with R and Python\n\n\nR Data.Tables: You can fetch query results from DuckDB into R data.tables, integrating with your existing R codebase.\nPython Polars: Similarly, you can read data into Polars DataFrames in Python, which is efficient for data manipulation.\n\n\nSimplified Data Pipeline\n\n\nUnified Data Source: Using DuckDB on top of Parquet files centralizes your data access, simplifying data management.\n\n\n\n5.1.7 Potential Disadvantages\n\nAdditional Complexity\n\n\nLibrary Dependencies: Requires installation and management of additional libraries (DuckDB, Parquet support in R and Python).\nLearning Curve for Integration: Even though you‚Äôre familiar with DuckDB, integrating it into R and Python workflows may require some setup and testing.\n\n\nOverhead of Data Conversion\n\nInitial Conversion: Migrating CSV files to Parquet adds an extra step in your data ingestion pipeline.\nData Updates: If your CSV data updates frequently, you‚Äôll need to automate the conversion process.\n\nResource Usage\n\n\nDisk Space: Maintaining both CSV and Parquet files (if not deleting the original CSVs) may consume additional storage.\n\n\nRecommendations and Best Practices\nGiven your requirements and skills, your proposed workflow is suitable and offers several benefits in terms of scalability and flexibility. Here are some recommendations to optimize your workflow:\n### 1. Automate the CSV to Parquet Conversion\n\nUse Batch Processing:\nCreate scripts in R or Python to automate the conversion of CSV files to Parquet.\nLeverage DuckDB for Conversion:\n\nDuckDB can read CSV files and write Parquet files, allowing you to perform the conversion within DuckDB.\n\n\nCOPY (SELECT * FROM 'your_data.csv') TO 'your_data.parquet' (FORMAT PARQUET);\n\n\n5.1.8 2. Optimize DuckDB Usage\n\nIndexing and Partitioning:\n\nWhile Parquet files do not support traditional indexing, consider partitioning your data to improve query performance.\n\nSQL Query Optimization:\n\nUse DuckDB‚Äôs advanced SQL features to optimize queries (e.g., window functions, common table expressions).\n\n\n\n\n5.1.9 3. Efficient Data Retrieval into R and Python\n\nIn R:\n\nUse the duckdb package to execute SQL queries and fetch results into data.tables.\n\nlibrary(duckdb)\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# Query data\nresult &lt;- dbGetQuery(con, \"SELECT * FROM 'your_data.parquet' WHERE conditions\")\n\n# Convert to data.table\nlibrary(data.table)\ndt_result &lt;- as.data.table(result)\nIn Python with Polars:\n\nUse DuckDB‚Äôs Python API or integrate with Polars for efficient data handling.\n\n\nimport duckdb\nimport polars as pl\n\n# Execute query and fetch result as Polars DataFrame\ndf = duckdb.query(\"SELECT * FROM 'your_data.parquet' WHERE conditions\").to_df()\npl_df = pl.from_pandas(df)\n\n\n5.1.10 4. Consider Data Volume and Hardware Resources\n\nMemory Management:\n\nDuckDB processes data efficiently, but ensure your hardware resources are adequate for your data size.\n\nDisk I/O:\n\nUsing Parquet files reduces disk I/O due to compression, but be mindful of the storage subsystem performance.\n\n\n\n\n5.1.11 5. Keep Libraries Updated\n\nStay Current:\n\nEnsure that you are using the latest versions of DuckDB, R packages, and Python libraries to benefit from performance improvements and bug fixes.\n\n\n\n\n5.1.12 6. Handle Updates and Data Versioning\n\nIncremental Updates:\n\nIf your data updates incrementally, design your pipeline to handle partial updates rather than reprocessing entire datasets.\n\nData Version Control:\n\nImplement versioning for your Parquet files to track changes over time.\n\n\n\n\n5.1.13 7. Monitor Performance and Adjust\n\nBenchmarking:\n\nRegularly benchmark query performance to identify bottlenecks.\n\nAdjust Strategy:\n\nBased on performance metrics, adjust partitioning schemes, query strategies, or hardware resources as needed.\n\n\n\nConclusion\nYour proposed workflow of migrating CSV files to Parquet, using DuckDB for querying, and integrating with R and Python is a robust solution that addresses your needs for scalability and flexibility. Given your familiarity with SQL and DuckDB, and the fact that you possess the necessary skills to manage the additional complexity, this approach is well-suited to your situation.\nBenefits of Your Workflow:\n\nScalable Data Processing:\nEfficient handling of large datasets through Parquet and DuckDB.\nFlexibility Across Languages:\n\nSeamless data access in both R and Python without data format conversion issues.\n\nPerformance Optimization:\n\nFaster query execution and data retrieval due to optimized storage and processing.\n\n\nKey Actions Moving Forward:\n\nSet Up Automated Data Pipelines:\n\n\nAutomate the conversion from CSV to Parquet to ensure consistency and save time.\n\n\nOptimize Queries:\n\n\nUtilize DuckDB‚Äôs full SQL capabilities to write efficient queries for your analysis.\n\n\nIntegrate Smoothly with R and Python:\n\nEstablish standard functions or scripts in both environments to interact with DuckDB, minimizing repetitive code.\n\nMonitor and Iterate:\n\nKeep an eye on performance metrics and be ready to adjust your approach as your dataset grows or changes.\n\n\n\nAdditional Considerations\n\nCommunity and Support:\n\nDuckDB is actively developed, and there is a growing community. Utilize resources like documentation and forums when needed.\n\nTesting and Validation:\n\nAs with any data pipeline, thoroughly test each component to ensure data integrity and correctness.\n\nSecurity and Access Control:\n\nIf working in a multi-user environment, consider how data access and permissions are managed.\n\n\n\nFinal Thoughts\nYour willingness to embrace additional complexity due to your skill set positions you well to benefit from this workflow. By combining the strengths of Parquet‚Äôs efficient data storage and DuckDB‚Äôs powerful query engine, you can achieve a scalable and flexible data analysis environment.\nShould you need further assistance or have more questions as you implement this workflow, feel free to reach out. I‚Äôm here to help ensure your data processing is as efficient and effective as possible.",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "nb/04_modelling_rev.html",
    "href": "nb/04_modelling_rev.html",
    "title": "4¬† Modelling",
    "section": "",
    "text": "4.1 blackbox models\nSee Figure Figure¬†4.1 for the thumbnail overview.",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "nb/04_modelling_rev.html#whitebox-models",
    "href": "nb/04_modelling_rev.html#whitebox-models",
    "title": "4¬† Modelling",
    "section": "4.2 whitebox models",
    "text": "4.2 whitebox models\n\n\n\n\n6.1. Literature Review / Similar Models - comprehensive overview of the state of the art in machine learning, including the latest algorithms, - insight into the performance of different algorithms and techniques on similar types of data. (avoid wasting time on models that are unlikely to perform well.)\n\n4.2.1 Model Selection\ndifferent models have different strengths and weaknesses and are suitable for different types of data and problems. achieve the best possible performance and maximize the impact\navoid overfitting a trade-off between the simplicity and flexibility (robustness)\nmodel performance - only article hierarchy - article and customer hierarchy\nCross-validation is a method for evaluating the performance of a model on a validation dataset. This involves dividing the data into k folds, training the model on k-1 folds, and evaluating the performance on the remaining fold. This process is repeated k times, with each fold serving as the validation set once, and the results are averaged to obtain a final performance score. This method provides a more reliable estimate of the model‚Äôs performance, as it uses all of the data for training and evaluation.\nalgorithm - handle teh specific characteristics of teh data - balance performance and interpretability - memory and processing power\n\nvalidity -&gt; fa & prediction intervals\nrobustness -&gt;\ntransparency -&gt;",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "nb/05_evaluation_rev.html",
    "href": "nb/05_evaluation_rev.html",
    "title": "5¬† Evaluation",
    "section": "",
    "text": "5.1 Evaluate Results",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "nb/05_evaluation_rev.html#evaluate-results",
    "href": "nb/05_evaluation_rev.html#evaluate-results",
    "title": "5¬† Evaluation",
    "section": "",
    "text": "explainability\n\n\n5.1.1 Assessment of Data Mining Results w.r.t. Business Success Criteria\n\nhow well does the model work on new data?\nFor whom does the model not work well?\n\n\n\n5.1.2 Approved Models\n\n\n5.1.3 Review Process\n\n\n5.1.4 Review of Process\n\n\n5.1.5 Determine Next Steps\n\n\n5.1.6 List of Possible Actions\n\n\n5.1.7 Decision",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "nb/06_deployment_rev.html",
    "href": "nb/06_deployment_rev.html",
    "title": "6¬† Deployment",
    "section": "",
    "text": "scalability - distributed techniques - maintenance & updating the model - monitoring the model - debugging the model\n\n7 model management\n\nmodel versioning GIT\ncontinuous integration/continuous deployment (CI/CD) pipelines.\n\n\n\nAppendix",
    "crumbs": [
      "Project Phases",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html",
    "href": "nb/08_appendix_rev.html",
    "title": "8¬† Other",
    "section": "",
    "text": "other",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html#sec-literature_review",
    "href": "nb/08_appendix_rev.html#sec-literature_review",
    "title": "8¬† Other",
    "section": "8.1 Literature",
    "text": "8.1 Literature\nThe literature review focuses on the intersection of three fields, according to the now-ubiquitous Data Science Venn Diagram of Drew Conway.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html#business-process",
    "href": "nb/08_appendix_rev.html#business-process",
    "title": "8¬† Other",
    "section": "8.2 Business Process",
    "text": "8.2 Business Process\n\nSCOR MODEL A COMPLETE GUIDE - 2020 EDITION, [@blokdyk_scor_2019]\nInventory and production management in supply chains, [@silver_inventory_2021]",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html#math-statistics",
    "href": "nb/08_appendix_rev.html#math-statistics",
    "title": "8¬† Other",
    "section": "8.3 Math & Statistics",
    "text": "8.3 Math & Statistics\n\nForecasting: Principles and Practice (3rd ed), [@hyndman_forecasting_2021]\nDemand forecasting best practices, [@vandeput_demand_2023]\nData science for supply chain forecasting, [@vandeput_data_2021]\nInventory optimization: models and simulations, [@vandeput_inventory_2020]\nMachine Learning With Boosting: A Beginner‚Äôs Guide, [@hartshorn_machine_2017]\nIntroduction to conformal prediction with Python, [@molnar_introduction_2023]\nPractical guide to applied conformal prediction in Python, [@manokhin_practical_2023]\nSHAP with Python, [@osullivan_shap_2024]\nIntroduction to SHAP with Python, [@osullivan_introduction_2023]\nEnsemble methods for machine learning, [@kunapuli_ensemble_2023]",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html#programming",
    "href": "nb/08_appendix_rev.html#programming",
    "title": "8¬† Other",
    "section": "8.4 Programming",
    "text": "8.4 Programming\n\n8.4.1 Python\n\nPython Crash Course, [@matthes2019]\nLightning fast forecasting with statistical and econometric models, [@nixtla/s2024]\nScalable Machine Learning for Time Series Forecasting, [@nixtla/m2024]\nProbabilistic hierarchical forecasting with statistical and econometric methods, [@nixtla/h]\nTS Features, calculates various features from time series data, [@nixtla/t2024]\n\n\n\n8.4.2 R\n\nR for Data Science, [@wickham]\nR forecasting package, [@hyndmanaut2024]\nR feasts package, [@ohara-wild2024a]\nR fable package, [@ohara-wild2024]",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html#sec-forecast-use-cases",
    "href": "nb/08_appendix_rev.html#sec-forecast-use-cases",
    "title": "8¬† Other",
    "section": "8.5 Forecast Use Cases",
    "text": "8.5 Forecast Use Cases\n\n\n\n\n\n\n\nForecast Use Cases",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html#sec-project-charter",
    "href": "nb/08_appendix_rev.html#sec-project-charter",
    "title": "8¬† Other",
    "section": "8.6 Project Charter",
    "text": "8.6 Project Charter\n\n\n\n\n\n\n\nProject Charter",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html#sec-project-plan",
    "href": "nb/08_appendix_rev.html#sec-project-plan",
    "title": "8¬† Other",
    "section": "8.7 Project Plan",
    "text": "8.7 Project Plan\n\n\n\n\n\n\n\nProject Plan\n\n\n\n\n\n\n\n\n\n\nProject Plan",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/08_appendix_rev.html#sec-eaisi",
    "href": "nb/08_appendix_rev.html#sec-eaisi",
    "title": "8¬† Other",
    "section": "8.8 EAISI",
    "text": "8.8 EAISI\n\n8.8.1 Deliverables & requirements\n\nClear description of the need / problem / opportunity\nClear description of the long-term ambition\nClear focus on what part of this long-term ambition will be taken on in the project\nDescription of the data science project with the help of a flowdown chart and project charter\nClear description of how the model outcome / analysis results will be used (e.g.¬†who, at what time, in which process, in what way, will use the model outcome / analysis results to make a better decision about what?) and how this translates into improved KPIs as outlined in the flowdown chart\nSubstantiated choice for a ‚Äòperformance metric‚Äô (i.e.¬†what should the model be good at)\nClear description of a business case / cost-benefit (no cost!!) analysis\nRealistic plan / timeline for how to execute the consecutive CRISP-DM phases",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "nb/16_flow_down_rev.html",
    "href": "nb/16_flow_down_rev.html",
    "title": "9¬† Flow Down",
    "section": "",
    "text": "9.1 Flow-down",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Flow Down</span>"
    ]
  },
  {
    "objectID": "nb/16_flow_down_rev.html#sec-flowdown",
    "href": "nb/16_flow_down_rev.html#sec-flowdown",
    "title": "9¬† Flow Down",
    "section": "",
    "text": "Flowdown Graph",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Flow Down</span>"
    ]
  },
  {
    "objectID": "nb/17_crisp-dm_rev.html",
    "href": "nb/17_crisp-dm_rev.html",
    "title": "10¬† CRISP-DM",
    "section": "",
    "text": "10.1 CRISP-DM",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>CRISP-DM</span>"
    ]
  },
  {
    "objectID": "nb/17_crisp-dm_rev.html#sec-crisp-dm",
    "href": "nb/17_crisp-dm_rev.html#sec-crisp-dm",
    "title": "10¬† CRISP-DM",
    "section": "",
    "text": "Project Plan",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>CRISP-DM</span>"
    ]
  },
  {
    "objectID": "nb/18_references_rev.html",
    "href": "nb/18_references_rev.html",
    "title": "11¬† References",
    "section": "",
    "text": "References\n\n\nBarrett, T, Dowle,\nM, Srinivasan, A,\nGorecki, J, Chirico,\nM, Hocking, T,\nSchwendinger, B,\nStetsenko, P, Short,\nT, Lianoglou, S,\nAntonyan, E, Bonsch,\nM, Parsonage, H,\nRitchie, S, Ren,\nK, Tan, X,\nSaporta, R, Seiskari,\nO, Dong, X,\nLang, M, Iwasaki,\nW, Wenchel, S,\nBroman, K, Schmidt,\nT, Arenburg, D,\nSmith, E, Cocquemas,\nF, Gomez, M,\nChataignon, P,\nBlaser, N, Selivanov,\nD, Riabushenko, A,\nLee, C, Groves,\nD, Possenriede, D,\nParages, F, Toth,\nD, Yaramaz-David, M,\nPerumal, A, Sams,\nJ, Morgan, M,\nQuinn, M, @javrucebo,\n@marc-outins, Storey,\nR, Saraswat, M,\nJacob, M, Schubmehl,\nM, Vaughan, D,\nSilvestri, L, Hester,\nJ, Damico, A,\nFreundt, S, Simons,\nD, Andrade, E S de,\nMiller, C, Meldgaard,\nJ P, Tlapak, V,\nUshey, K,\nEddelbuettel, D,\nFischetti, T, Shilon,\nO, Khotilovich, V,\nWickham, H, Becker,\nB, Haynes, K,\nKamgang, B C,\nDelmarcell, O,\nO‚ÄôBrien, J, Mezquita,\nD de, Czekanski, M,\nShemetov, D, Jha,\nN, Wu, J,\nGin√©-V√°zquez, I,\nChetia, A,\nAmoakohene, D, and\nKrylov, I 2024 Data.table:\nExtension of ‚Äôdata.frame‚Äô. Available at https://cran.r-project.org/web/packages/data.table/index.html\n[Last accessed 13 October 2024].\n\n\nBLOKDYK, G 2019 SCOR\nMODEL A COMPLETE\nGUIDE - 2020 EDITION. [S.l.]: 5STARCOOKS.\n\n\nCore Team}, {R 2024 R: A\nLanguage and Environment for\nStatistical Computing. Available at https://www.R-project.org/\n\n\nCosta, R 2022 The\nCRISP-ML Methodology:\nA Step-by-Step\nApproach to Real-World\nMachine Learning Projects.\nIndependently published.\n\n\nDeBruine, L 2023 Glossary:\nGlossaries for Markdown and\nQuarto Documents. Available at https://cran.r-project.org/web/packages/glossary/\n[Last accessed 13 October 2024].\n\n\nGuja, A and Siwiak,\nM 2025 Generative AI for\nData Analytics. Manning Publications Co.\nLLC 2025.\n\n\nHartshorn, S 2017 Machine\nLearning With Boosting:\nA Beginner‚Äôs Guide.\n\n\nHyndman [aut, R, cre,\ncph, Athanasopoulos,\nG, Bergmeir, C,\nCaceres, G, Chhay,\nL, Kuroptev, K,\nO‚ÄôHara-Wild, M,\nPetropoulos, F,\nRazbash, S, Wang,\nE, Yasmeen, F,\nGarza, F,\nGirolimetto, D,\nIhaka, R, R Core\nTeam, Reid, D,\nShaub, D, Tang,\nY, Wang, X, and\nZhou, Z 2024a Forecast:\nForecasting functions for time series and linear models. Available\nat https://cran.r-project.org/web/packages/forecast/index.html\n\n\nHyndman [aut, R, cre,\ncph, Athanasopoulos,\nG, O‚ÄôHara-Wild, M,\nPalihawadana, N,\nWickramasuriya, S, and\nRStudio 2024b fpp3: Data for\n\"Forecasting: Principles and\nPractice\" (3rd Edition). Available at https://cran.r-project.org/web/packages/fpp3/index.html\n[Last accessed 13 October 2024].\n\n\nHyndman, R J and\nAthanasopoulos, G 2021\nForecasting: Principles and Practice (3rd\ned). Available at https://otexts.com/fpp3/ [Last\naccessed 5 October 2024].\n\n\nKunapuli, G 2023 Ensemble methods\nfor machine learning. Shelter Island, NY: Manning.\n\n\nManokhin, V 2023 Practical guide\nto applied conformal prediction in Python: Learn and apply\nthe best uncertainty frameworks to your industry applications.\nBirmingham Mumbai: &lt;packt&gt;.\n\n\nMatthes, E 2019b Python crash\ncourse: A hands-on, project-based introduction to programming. 2nd\nedition. San Francisco, CA: No Starch Press.\n\n\nMatthes, E 2019a Python crash\ncourse: A hands-on, project-based introduction to programming. 2nd\nedition. San Francisco, CA: No Starch Press.\n\n\nMolnar, C 2023 Introduction to\nconformal prediction with Python: A short guide for\nquantifying uncertainty of machine learning models. First edition.\nM√ºnchen, Germany: Chistoph Molnar c/o MUCBOOK, Heidi Seibold. Available\nat https://christophmolnar.com/books/conformal-prediction/\n\n\nNixtla/hierarchicalforecast: Probabilistic hierarchical forecasting üëë\nwith statistical and econometric methods. n.d. Available at https://github.com/Nixtla/hierarchicalforecast\n\n\nNixtla/mlforecast 2024. Nixtla. Available at https://github.com/Nixtla/mlforecast\n\n\nNixtla/statsforecast 2024. Nixtla. Available at https://github.com/Nixtla/statsforecast\n\n\nNixtla/tsfeatures 2024. Nixtla. Available at https://github.com/Nixtla/tsfeatures\n\n\nO‚ÄôHara-Wild, M,\nHyndman, R, Wang,\nE, Cook, D,\nfeatures), T T (Correlation, and\nmethod), L C (Guerrero‚Äôs 2024a\nFeasts: Feature extraction and statistics for time series.\nAvailable at https://cran.r-project.org/web/packages/feasts/index.html\n\n\nO‚ÄôHara-Wild, M,\nHyndman, R, Wang,\nE, implementation), G C\n(NNETAR, Bergmeir, C,\nHensel, T-G, and\nHyndman, T 2024b Fable:\nForecasting models for tidy time series. Available at https://cran.r-project.org/web/packages/fable/index.html\n\n\nO‚ÄôSullivan, C 2024 SHAP\nwith Python. SHAP with Python. Available at https://adataodyssey.com/course/\n\n\nO‚ÄôSullivan, C 2023 Introduction to\nSHAP with Python. Introduction to SHAP\nwith Python. Available at https://towardsdatascience.com/introduction-to-shap-with-python-d27edc23c454\n\n\nPorter, L and\nZingaro, D 2024 Learn\nAI-assisted Python programming: With\nGitHub Copilot and ChatGPT.\nShelter Island, NY: Manning.\n\n\nSilver, E A, Pyke,\nD F, and Thomas, D J\n2021 Inventory and production management in supply chains.\nFourth edition, first issued in paperback. Boca Raton, FL London New\nYork, NY: CRC Press, Taylor & Francis Group.\n\n\nVandeput, N 2023 Demand\nforecasting best practices. Shelter Island, NY: Manning\nPublications Co.\n\n\nVandeput, N 2021 Data science for\nsupply chain forecasting. Second edition. Berlin ; Boston: De\nGruyter.\n\n\nVandeput, N 2020 Inventory\noptimization: Models and simulations. Berlin ; Boston: De Gruyter.\n\n\nWickham, H n.d. R for data science\n(2e). Available at https://r4ds.hadley.nz/\n\n\nWickham, H, Chang,\nW, Henry, L,\nPedersen, T L,\nTakahashi, K, Wilke,\nC, Woo, K,\nYutani, H,\nDunnington, D, Brand,\nT van den, Posit, and\nPBC 2024 ggplot2: Create\nElegant Data Visualisations\nUsing the Grammar of Graphics.\nAvailable at https://cran.r-project.org/web/packages/ggplot2/index.html\n[Last accessed 13 October 2024].",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "nb/19_resources_rev.html",
    "href": "nb/19_resources_rev.html",
    "title": "12¬† Resources",
    "section": "",
    "text": "Resources\n\nDownload the Business Understanding notebook (QMD) üíæ\nDownload the Data Understanding notebook (QMD) üíæ\nDownload EAISI Graduation Presentation (PPTX) üíæ\nDownload all notebooks (ZIP) üíæ",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Resources</span>"
    ]
  }
]